{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH 582 Final Project\n",
    "Author: Jace Kline 2881618\n",
    "\n",
    "### Project Description & Motivation\n",
    "\n",
    "In this project, I aim to use health and fitness metrics scraped from my personal Garmin Connect account to analyze correlational relationships and perform dimensionality reduction, regression, classification, and clustering. The data is scraped from the time period of early June through late November and additionally exist across different time scales - daily and weekly. The metrics observed in the data sets include Calories burned, stress level, resting heart rate (BPM), steps, intensity minutes, and sleep hours. I chose this project because of my recent entrance into the world of endurance sports. I aim to leverage data science techniques learned in this class to derive useful and interesting results for myself as I continue on my endurance sports journey.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as mp\n",
    "import seaborn as sb\n",
    "from IPython.display import display\n",
    "\n",
    "# dimensionality reduction, feature selection, data splitting\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# regression\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, ARDRegression\n",
    "\n",
    "# classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned data sets\n",
    "dataloc = \"./data/cleaned/\"\n",
    "\n",
    "daily = pd.read_csv(dataloc + \"daily.csv\").set_index('Date')\n",
    "weekly = pd.read_csv(dataloc + \"weekly.csv\").set_index('Week')\n",
    "\n",
    "dataset_labels = [\"Daily\", \"Weekly\"]\n",
    "datasets = [daily, weekly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_datasets(procedure):\n",
    "    for df, lbl in zip(datasets, dataset_labels):\n",
    "        procedure(df, lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets\n",
    "\n",
    "In this section, we will describe and show the data sets used in the project. As discussed previously, the data comes from Garmin Connect and exists across daily and weekly time granularities. Hence, we utilize a daily and weekly data set. The data sets and their associated features are listed below:\n",
    "\n",
    "* Daily metrics\n",
    "  * Date\n",
    "  * Intensity Minutes - the amount of time in a given day spent doing intense exercise\n",
    "  * Steps - the number of steps taken in a given day\n",
    "  * BPM - the average resting heart rate for a given day\n",
    "  * Calories - the total number of calories burned in a given day\n",
    "  * Stress Level - a Garmin score indicating the level of stress exhibited in a given day\n",
    "  * Sleep Hrs - the number of hours slept for a given night\n",
    "  * Sleep Hrs Prev - the number of hours slept the previous night\n",
    "\n",
    "\n",
    "* Weekly metrics\n",
    "  * Week - the date marking the first day of a seven day period\n",
    "  * Intensity Minutes - the total intensity minutes in a given week\n",
    "  * Steps - the average daily steps for a given week\n",
    "  * BPM - the average resting heart rate over a given week\n",
    "  * Calories - the total number of calories burned in a given week\n",
    "  * Stress Level - The average daily stress level (Garmin score) over a given week\n",
    "  * Sleep Hrs - The average number of hours slept per night over the course of a given week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the structure of the data sets\n",
    "\n",
    "display(daily)\n",
    "display(weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation\n",
    "\n",
    "In this section, we aim to show the correlation plots of features in our datasets that feature metrics broken down by day, week, and month. These different levels of granularity offer unique insights into the relationship of our different health metrics over the course of time. The results that we obtain in this section will help us narrow the scope and focus of our further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_correlations(df, lbl):\n",
    "    print(f\"{lbl} Correlations:\")\n",
    "    corr = df.corr()\n",
    "    display(corr)\n",
    "    \n",
    "    sb.heatmap(corr, cmap=\"YlGnBu\", annot=True)\n",
    "    mp.show()\n",
    "\n",
    "# for lbl, df in zip(dataset_labels, datasets):\n",
    "#     display_correlations(df, lbl)\n",
    "\n",
    "iterate_datasets(display_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "These correlation plots show us that the number of hours of sleep per night are negatively correlated with all other features. This is most pronounced on the weekly scale as opposed to the daily scale. Additionally, we see that stress level and resting heart rate (BPM) are significantly positively correlated in both the daily and weekly time scales. This indicates that both acute and prolonged stress associate with an increase in heart rate. Intuitively, we see that steps, intensity minutes, and Calories burned are all significantly positively correlated. The overlapping information stored within all these features present the possible success of dimensionality reduction on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction (PCA & SVD)\n",
    "\n",
    "As touched on in the feature correlation discussion, the shared information and high correlations present between features in the data present the possible success of dimensionality reduction using principal component analysis (PCA) and singular value decomposition (SVD) on the data. The dimension-reduced data sets can then be utilized in further models leveraging these data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD Reducer function\n",
    "\n",
    "# X: Data set with D columns (features), N rows (samples)\n",
    "# M: the number of dimensions to reduce to where M <= D\n",
    "def svd_reduce(X, M):\n",
    "    U, singular_values, Vt = np.linalg.svd(X.T, full_matrices=False, compute_uv=True)\n",
    "    Sigma = np.diag(singular_values)\n",
    "    return (U[:,0:M] @ Sigma[0:M,0:M] @ Vt[0:M,:]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Reducer function\n",
    "\n",
    "# X: Data set with D columns (features), N rows (samples)\n",
    "# M: the number of dimensions to reduce to where M <= D\n",
    "# transforms the shape of data matrix back to original space\n",
    "def pca_reduce(X, M):\n",
    "    pca = PCA(n_components=M)\n",
    "    return pca.inverse_transform(pca.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_vs_pca_err(X):\n",
    "    N, D = X.shape\n",
    "    results = []\n",
    "\n",
    "    for M in range(1, D):\n",
    "        svd_err = np.linalg.norm(X - svd_reduce(X, M))\n",
    "        pca_err = np.linalg.norm(X - pca_reduce(X, M))\n",
    "        results.append((M, svd_err, pca_err))\n",
    "\n",
    "    return pd.DataFrame(results, columns=['Dimensions', 'SVD Error', 'PCA Error']).set_index('Dimensions')\n",
    "\n",
    "def dim_reduction_results(df, lbl):\n",
    "    print(f\"SVD vs PCA on the {lbl} data set:\")\n",
    "    res = svd_vs_pca_err(df.to_numpy())\n",
    "    display(res)\n",
    "    res.plot(kind='bar', logy=True)\n",
    "    mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate_datasets(dim_reduction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The results above indicate to us that dimensionality reduction on our target data sets is feasible and fairly accurate when using at least 3 dimensions. We utilized the Frobenius Norm function to measure the error the PCA and SVD methods. We see that the the PCA reduction resulted in more accurate information preservation than that of the SVD reduction. The success of PCA in this case can be explained by the relatively high redundancy, and therefore covariance, exhibited via the similarities in features such as Calories burned and intensity minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "In this section, we aim to utilize regression to predict each feature using the other features. To achieve successful regression results, we must first prune our data set features to only utilize features that result in useful predictive outcomes for the given problem at hand. To achieve this, we will utilize the forward sequential feature selector offered by SciKit Learn to test different combinations of features on a given regression input model. We will choose the set of features that performs the highest for the given model. We will iterate over each of the data sets and perform regression against each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input DataFrame (df) into X and y based on the target regression feature name (y_feature)\n",
    "def split_dataset(df, y_feature):\n",
    "    \n",
    "    X_features = np.array(df.columns)\n",
    "    X_features = X_features[X_features != y_feature]\n",
    "\n",
    "    X = df[X_features]\n",
    "    y = df[y_feature]\n",
    "    return X, y\n",
    "\n",
    "# Find the optimal features for the given target\n",
    "# return the optimal features, the score of the model on the training data, and the score of the model on the testing data\n",
    "def evaluate_model(X, y, model):\n",
    "    \n",
    "    feature_selector = SequentialFeatureSelector(model, k_features='best', n_jobs=-1)\n",
    "\n",
    "    feature_selector.fit(X, y)\n",
    "    optimal_features = list(feature_selector.k_feature_names_)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[optimal_features], y, random_state=57)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "\n",
    "    return optimal_features, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SciKit Learn regression models to test\n",
    "model_labels = [\"LinearRegression\", \"Lasso\", \"Ridge\", \"ElasticNet\", \"ARDRegression\"]\n",
    "models = [LinearRegression(), Lasso(), Ridge(), ElasticNet(), ARDRegression()]\n",
    "\n",
    "# output the results of running all of the above regression models on the input data set and target feature\n",
    "def regression_tests(df, df_lbl, y_feature):\n",
    "    results = []\n",
    "    X, y = split_dataset(df, y_feature)\n",
    "\n",
    "    for model, lbl in zip(models, model_labels):\n",
    "        start = time.time()\n",
    "        ftrs, train_score, test_score = evaluate_model(X, y, model)\n",
    "        time_taken = time.time() - start\n",
    "        results.append((lbl, ftrs, train_score, test_score, time_taken))\n",
    "\n",
    "    res_table = pd.DataFrame(results, columns=['Model Type', 'Feature Inputs', 'Train Score', 'Test Score', 'Time Taken']).set_index('Model Type') \n",
    "    print(\"Feature Selector: Forward Sequential\")\n",
    "    print(f\"Data Set: {df_lbl}\")\n",
    "    print(f\"Prediction Feature: {y_feature}\")\n",
    "    display(res_table)\n",
    "    res_table[['Train Score', 'Test Score']].plot(\n",
    "        kind='bar',\n",
    "        title=f\"Results of predicting feature '{y_feature}' on the {df_lbl} data set\"\n",
    "    )\n",
    "    mp.show()\n",
    "    return res_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Intensity Minutes', 'Calories', 'Stress Level', 'Sleep Hrs']\n",
    "results = []\n",
    "\n",
    "# Iterate over the data sets\n",
    "for df, df_lbl in zip(datasets, dataset_labels):\n",
    "    # For each data set, perform regression tests on each of the features\n",
    "    for y_feature in targets:\n",
    "        res = regression_tests(df, df_lbl, y_feature)\n",
    "        results.append((f\"{y_feature} ({df_lbl})\", res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and show the results for the models across all runs\n",
    "# show distinct results for predictions on the training and testing data\n",
    "test_lbls = [test_lbl for test_lbl, _ in results]\n",
    "for feature in ['Train Score', 'Test Score', 'Time Taken']:\n",
    "    results_arr = np.vstack([res[feature].to_numpy() for _, res in results])\n",
    "    model_performances = pd.DataFrame(results_arr, columns=model_labels, index=test_lbls)\n",
    "    avgs = model_performances.mean()\n",
    "    best_performer = (min if feature == 'Time Taken' else max)(zip(list(avgs.index), list(avgs)), key=lambda t: t[1])[0]\n",
    "\n",
    "    print(f\"\\n\\nMeasurement: {feature}\")\n",
    "    display(model_performances)\n",
    "\n",
    "    print(f\"Model Average Performances ({feature})\")\n",
    "    display(avgs)\n",
    "    print(f\"Best Average Performer ({feature}): {best_performer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The above results show us that some of the features are much better suited to prediction via a regression model. Particularly, predicting the following metrics lead to respectable results: daily intensity minutes, daily Calories, and weekly stress level. The lack of accuracy in performing regression across the other features displays the fact that our data lacks necessary information required for us to make reasonable predictions. Data is simply a rough snapshot of a subset of important factors that could dictate the outcome of a phenomemon in the real world, and it is evident that predicting factors such as sleep hours, resting heart rate, and other features requires additional information not captured in our data.\n",
    "\n",
    "Another interesting observation that arises from our results above is the fact that the number of optimal features in each of the regression models is nearly always a proper subset of the total set of features. This implies that some of the features or combinations of the features being utilized in the model may actually result in reduced accuracy through overfitting or addition of noise. This highlights the fact that more features are not always better when constructing a model.\n",
    "\n",
    "Lastly, we show the comparison of regression predictions across the different models used. This analysis highlights the fact that the models all behaved similarly across the test runs. However, the LinearRegression and Ridge models behaved best overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "In this section, we will utilize binary classification to attempt to determine whether a feature value lies above or below a given threshold placed at the median of the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_classification(df, y_feature):\n",
    "    X, y = split_dataset(df, y_feature)\n",
    "    threshold = y.median()\n",
    "    y_classification = (y >= threshold).apply(lambda b: 1 if b else -1)\n",
    "    return X, y, y_classification, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SciKit Learn classification models to test\n",
    "svc_kernels = ['poly', 'rbf']\n",
    "model_labels = [f'SVC (kernel = {kernel})' for kernel in svc_kernels] +\\\n",
    "    [\"GaussianNB\", \"MultinomialNB\", \"SGDClassifier\", \"DecisionTreeClassifier\", \"RandomForestClassifier\"]\n",
    "models = [SVC(kernel=kernel) for kernel in svc_kernels] +\\\n",
    "    [GaussianNB(), MultinomialNB(), SGDClassifier(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "\n",
    "# output the results of running all of the above regression models on the input data set and target feature\n",
    "def classification_tests(df, df_lbl, y_feature):\n",
    "    results = []\n",
    "    X, y, y_classification, threshold = split_dataset_classification(df, y_feature)\n",
    "\n",
    "    for model, lbl in zip(models, model_labels):\n",
    "        start = time.time()\n",
    "        ftrs, train_score, test_score = evaluate_model(X, y_classification, model)\n",
    "        time_taken = time.time() - start\n",
    "        results.append((lbl, ftrs, train_score, test_score, time_taken))\n",
    "\n",
    "    res_table = pd.DataFrame(results, columns=['Model Type', 'Feature Inputs', 'Train Score', 'Test Score', 'Time Taken']).set_index('Model Type') \n",
    "    print(\"Feature Selector: Forward Sequential\")\n",
    "    print(f\"Data Set: {df_lbl}\")\n",
    "    print(f\"Prediction Feature: {y_feature}\")\n",
    "    print(f\"Classification Threshold: {threshold}\")\n",
    "    display(res_table)\n",
    "    res_table[['Train Score', 'Test Score']].plot(\n",
    "        kind='bar',\n",
    "        title=f\"Results of classifying feature '{y_feature}' on the {df_lbl} data set above or below threshold {threshold}\"\n",
    "    )\n",
    "    mp.show()\n",
    "    return res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Intensity Minutes', 'Calories', 'Stress Level', 'Sleep Hrs']\n",
    "results = []\n",
    "\n",
    "# Iterate over the data sets\n",
    "for df, df_lbl in zip(datasets, dataset_labels):\n",
    "    # For each data set, perform regression tests on each of the features\n",
    "    for y_feature in targets:\n",
    "        res = classification_tests(df, df_lbl, y_feature)\n",
    "        results.append((f\"{y_feature} ({df_lbl})\", res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and show the results for the models across all runs\n",
    "# show distinct results for predictions on the training and testing data\n",
    "test_lbls = [test_lbl for test_lbl, _ in results]\n",
    "for feature in ['Train Score', 'Test Score', 'Time Taken']:\n",
    "    results_arr = np.vstack([res[feature].to_numpy() for _, res in results])\n",
    "    model_performances = pd.DataFrame(results_arr, columns=model_labels, index=test_lbls)\n",
    "    avgs = model_performances.mean()\n",
    "    best_performer = (min if feature == 'Time Taken' else max)(zip(list(avgs.index), list(avgs)), key=lambda t: t[1])[0]\n",
    "\n",
    "    print(f\"\\n\\nMeasurement: {feature}\")\n",
    "    display(model_performances)\n",
    "\n",
    "    print(f\"Model Average Performances ({feature})\")\n",
    "    display(avgs)\n",
    "    print(f\"Best Average Performer ({feature}): {best_performer}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09caa65032656f8918f2230d0aa21fe0157b183f7a129b6159c897ffe6591440"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
